{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPStNofuBFoaeW+yykDpVTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anh1811/trajectory-prediction/blob/main/LSTM_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mSbx4m8x59aH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader,  SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from time import time\n",
        "from prettytable import PrettyTable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GOS6ENvS6Dn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0549cbea-73e2-4181-d69a-6ee30e8821ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWZtWHeO2bBN"
      },
      "source": [
        "#DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pWSbFm6qlX",
        "outputId": "335ee275-afe2-4a00-cdcd-8802dda04e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# %cd content\n",
        "# %cd /content/drive/MyDrive/viettle/miniproject/sequencer\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MAN643GQ43Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f5070d-bdf9-4199-b4b2-2244932e2701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.83s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.60s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "root_dir = 'tracking'\n",
        "coco_annotation_file_path = os.path.join(root_dir, 'train.json')\n",
        "coco_test_annotation_file_path = os.path.join(root_dir, 'val.json')\n",
        "coco_annotation = COCO(annotation_file=coco_annotation_file_path)\n",
        "coco_test_annotation = COCO(annotation_file=coco_test_annotation_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2ARi1mKvBfKb"
      },
      "outputs": [],
      "source": [
        "def xywh2xyxy(bbox):\n",
        "  return bbox[0], bbox[1], bbox[2] + bbox[0], bbox[3] + bbox[1]\n",
        "\n",
        "\n",
        "def xywh2cxcy(bbox):\n",
        "  return bbox[0] + bbox[2]/2.0, bbox[1] + bbox[3]/2.0, bbox[2], bbox[3]\n",
        "\n",
        "def Euclipe_dis(bbox1, bbox2):\n",
        "  return np.sqrt((bbox1[0] - bbox2[0])**2 + (bbox1[1] - bbox2[1])**2)\n",
        "\n",
        "def scale(bbox, h_pic = 1920, w_pic = 2560):\n",
        "  if len(bbox) == 2:\n",
        "    return bbox[0]/w_pic, bbox[1]/h_pic\n",
        "  else: \n",
        "    return bbox[0]/w_pic, bbox[1]/h_pic, bbox[2]/w_pic, bbox[3]/h_pic\n",
        "\n",
        "def checknois(bboxes):\n",
        "  num_noise = 0\n",
        "  for i in range(len(bboxes)-1):\n",
        "    if Euclipe_dis(bboxes[i], bboxes[i+1]) > 0.1:\n",
        "      num_noise += 1\n",
        "  if num_noise >= 2:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def xywh2cxcyah(bbox):\n",
        "  return bbox[0] + bbox[2]/2.0, bbox[1] + bbox[3]/2.0, bbox[3]/bbox[2], bbox[3]\n",
        "\n",
        "def batch_xyxy2cxcy(bboxes):\n",
        "  batch_size = bboxes.size(0)\n",
        "  bbox_cxcy = torch.empty((batch_size,2), dtype=torch.float32)\n",
        "  bbox_cxcy[:,0] = (bboxes[:,0] + bboxes[:,2])/2.\n",
        "  bbox_cxcy[:,1] = (bboxes[:,1] + bboxes[:,3])/2.\n",
        "  return bbox_cxcy\n",
        "\n",
        "def vid_path(x):\n",
        "  return x[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L7056TFh4-uQ"
      },
      "outputs": [],
      "source": [
        "def create_list_ID(coco_annotation, coco_test_annotation):\n",
        "  list_ID = dict()\n",
        "  vid_id = -1\n",
        "  list_bbox = coco_annotation.getAnnIds()\n",
        "  list_test_bbox = coco_test_annotation.getAnnIds()\n",
        "  list_overall = list_bbox + list_test_bbox\n",
        "  list_overall.sort()\n",
        "  \n",
        "  for i,bbox in enumerate(list_overall):\n",
        "    try:\n",
        "      bbox_info = coco_annotation.loadAnns([bbox])[0]\n",
        "    except:\n",
        "      bbox_info = coco_test_annotation.loadAnns([bbox])[0] \n",
        "    track_id = bbox_info['attributes']['track_id'] \n",
        "    try:\n",
        "      img = coco_annotation.loadImgs([bbox_info['image_id']])[0]\n",
        "    except:\n",
        "      img = coco_test_annotation.loadImgs([bbox_info['image_id']])[0]\n",
        "    vid_path, img_path = img[\"file_name\"].split('/')\n",
        "    if i == 0 or vid_path != old_vid_path: \n",
        "      vid_id += 1\n",
        "      old_vid_path = vid_path\n",
        "    label = vid_path + \"_{}\".format(track_id)\n",
        "    if label in list_ID.keys():\n",
        "      list_ID[label].append([bbox, img_path])\n",
        "    else:\n",
        "      list_ID[label] = list()\n",
        "      list_ID[label].append([bbox, img_path])\n",
        "  return list_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ANZEwBSxknhy"
      },
      "outputs": [],
      "source": [
        "def create_train_list(list_ID, len_seq):\n",
        "  train_list = []\n",
        "  for items in list_ID.items():\n",
        "    id_for_one_obj = items[1]\n",
        "    len_items = len(id_for_one_obj) \n",
        "    if len_items >= len_seq:\n",
        "      for i,id in enumerate(id_for_one_obj[:-len_seq - 1]):\n",
        "          train_list.append(list(id_for_one_obj[i:i+len_seq]))\n",
        "      train_list.append(list(id_for_one_obj[-len_seq:]))\n",
        "  return train_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5ng_e8QCE0HU"
      },
      "outputs": [],
      "source": [
        "def create_dataset(train_list, coco_annotation, coco_test_annotation, type_box = 'xyxy', transformer = False):\n",
        "  datas = list()\n",
        "  labels = list()\n",
        "  for seq in train_list:\n",
        "    bbox_list = list()\n",
        "    for pre in seq[:-1]:\n",
        "      try:\n",
        "        bbox_list.append(coco_annotation.loadAnns(pre)[0])\n",
        "      except:\n",
        "        bbox_list.append(coco_test_annotation.loadAnns(pre)[0])\n",
        "    bbox_cxcy = [scale(xywh2cxcy(attr['bbox'])) for attr in bbox_list]\n",
        "    bbox_seq = [scale(xywh2xyxy(attr['bbox'])) for attr in bbox_list]\n",
        "    if np.var(bbox_cxcy) > 1e-5 and not checknois(bbox_cxcy):\n",
        "      # bbox_seq = [xywh2xyxy(attr['bbox']) for attr in bbox_list]\n",
        "      try:\n",
        "        label = coco_annotation.loadAnns([seq[-1]])[0]['bbox']\n",
        "      except:\n",
        "        label = coco_test_annotation.loadAnns([seq[-1]])[0]['bbox']\n",
        "      if type_box == 'xyxy':\n",
        "        datas.append(bbox_seq)\n",
        "        if transformer:\n",
        "          label = bbox_seq[1:] + [scale(xywh2xyxy(label))]\n",
        "        else:\n",
        "          label = scale(xywh2xyxy(label))\n",
        "        labels.append(label)\n",
        "      elif type_box == 'cxcy':\n",
        "        datas.append(bbox_cxcy)\n",
        "        labels.append(scale(xywh2cxcy(label)))\n",
        "      \n",
        "  return np.array(datas, dtype='float32'), np.array(labels, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(X, y, batch_size = 128):\n",
        "  inputs = torch.tensor(X)\n",
        "  labels = torch.tensor(y)\n",
        "\n",
        "  data = TensorDataset(inputs, labels)\n",
        "  sampler = SequentialSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler,batch_size=batch_size)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "jedLMcuEEacY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "-gUoeiAW4IOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_infer(model, batch_size, x_train, y_train):\n",
        "  device = torch.device(\"cuda\")\n",
        "  dummy_input = torch.randn(1,5,4, dtype=torch.float).to(device)\n",
        "  data_test = list()\n",
        "  starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "  batch_size = batch_size \n",
        "  infer_time = list()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _ in range(10):\n",
        "      _ = model(dummy_input)\n",
        "    for i, batch in enumerate(batch_size):\n",
        "      train_loader = dataloader(x_train, y_train, batch)\n",
        "      data_iter = iter(train_loader)\n",
        "      frame = next(data_iter)\n",
        "      data_test.append(frame[0])    \n",
        "      data = data_test[i].cuda()\n",
        "      time = list()\n",
        "      for rep in range(100):\n",
        "        starter.record()\n",
        "        _ = model(data)\n",
        "        ender.record()\n",
        "        # WAIT FOR GPU SYNC\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        time.append(curr_time)\n",
        "      infer_time.append(np.sum(time)/100)\n",
        "  return [time/batch*1000 for (time, batch) in zip(infer_time, batch_size)], data_test"
      ],
      "metadata": {
        "id": "QuQposysx-FW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generalized_iou(gt_bboxes, pr_bboxes, reduction='mean'):\n",
        "    \"\"\"\n",
        "    gt_bboxes: tensor (-1, 4) xyxy\n",
        "    pr_bboxes: tensor (-1, 4) xyxy\n",
        "    loss proposed in the paper of giou\n",
        "    \"\"\"\n",
        "    gt_area = (gt_bboxes[:, 2]-gt_bboxes[:, 0])*(gt_bboxes[:, 3]-gt_bboxes[:, 1])\n",
        "    pr_area = (pr_bboxes[:, 2]-pr_bboxes[:, 0])*(pr_bboxes[:, 3]-pr_bboxes[:, 1])\n",
        "\n",
        "    # iou\n",
        "    lt = torch.max(gt_bboxes[:, :2], pr_bboxes[:, :2])\n",
        "    rb = torch.min(gt_bboxes[:, 2:], pr_bboxes[:, 2:])\n",
        "    TO_REMOVE = 0\n",
        "    wh = (rb - lt + TO_REMOVE).clamp(min=0)\n",
        "    inter = wh[:, 0] * wh[:, 1]\n",
        "    union = gt_area + pr_area - inter\n",
        "    iou = inter / union\n",
        "    # # enclosure\n",
        "    # lt = torch.min(gt_bboxes[:, :2], pr_bboxes[:, :2])\n",
        "    # rb = torch.max(gt_bboxes[:, 2:], pr_bboxes[:, 2:])\n",
        "    # wh = (rb - lt + TO_REMOVE).clamp(min=0)\n",
        "    # enclosure = wh[:, 0] * wh[:, 1]\n",
        "\n",
        "    # giou = iou - (enclosure-union)/enclosure\n",
        "    # loss = 1. - giou\n",
        "    if reduction == 'mean':\n",
        "        iou = iou.mean()\n",
        "    elif reduction == 'sum':\n",
        "        iou =iou.sum()\n",
        "    elif reduction == 'none':\n",
        "        pass\n",
        "    return iou"
      ],
      "metadata": {
        "id": "6wkydLkH42Mb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n"
      ],
      "metadata": {
        "id": "VFyEEfcW6qrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, units = 128):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_linear = nn.Linear(self.hidden_size * 2, units)\n",
        "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        self.other = torch.FloatTensor(1, hidden_size)\n",
        "\n",
        "    def forward(self, outputs, final_hidden_state):\n",
        "        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\n",
        "        # final_hidden_state.shape: (batch_size, hidden_size)\n",
        "        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)\n",
        "        batch_size, seq_len, _ = outputs.shape\n",
        "        attn_weights = self.attn(outputs) # (batch_size, seq_len, hidden_dim)\n",
        "        attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
        "        attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\n",
        "\n",
        "        context = torch.bmm(outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
        "\n",
        "        return attn_hidden\n",
        "\n",
        "\n",
        "class LSTM_Attention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTM_Attention, self).__init__()\n",
        "\n",
        "        # Defining the number of nodes in each layer\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm1 = nn.LSTM(input_dim, hidden_dim,  batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first = True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.leakyRelu = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0, c0 = self.init_hidden(x)\n",
        "        out1, (h1, c1) = self.lstm1(x, (h0, c0))\n",
        "        out1 = self.dropout(out1)\n",
        "        out2, (h2, c2) = self.lstm2(out1, (h1, c1))\n",
        "        out3, (h3,c3) = self.lstm3(out2, (h2,c2))\n",
        "        out3 = self.dropout(out3)\n",
        "        final_state = h3.squeeze(0)\n",
        "        out = self.attention(out3, final_state)\n",
        "        out = self.fc1(out)\n",
        "        out = self.leakyRelu(out)\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def init_hidden(self, x):\n",
        "        # Initializing hidden state for first input with zeros\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_dim)\n",
        "        # Initializing cell state for first input with zeros\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_dim)\n",
        "        return [t.cuda() for t in (h0, c0)]"
      ],
      "metadata": {
        "id": "2Iil0eyp6tKb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss \n"
      ],
      "metadata": {
        "id": "2euv49foxrBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import warnings\n",
        "def ciou_loss(pred, target, eps=1e-6, reduction='mean'):\n",
        "    \"\"\"`Implementation of paper `Enhancing Geometric Factors into\n",
        "    Model Learning and Inference for Object Detection and Instance\n",
        "    Segmentation <https://arxiv.org/abs/2005.03572>`_.\n",
        "\n",
        "    Code is modified from https://github.com/Zzh-tju/CIoU.\n",
        "\n",
        "    Args:\n",
        "        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),\n",
        "            shape (n, 4).\n",
        "        target (Tensor): Corresponding gt bboxes, shape (n, 4).\n",
        "        eps (float): Eps to avoid log(0).\n",
        "    Return:\n",
        "        Tensor: Loss tensor.\n",
        "    \"\"\"\n",
        "    # overlap\n",
        "    lt = torch.max(pred[:, :2], target[:, :2])\n",
        "    rb = torch.min(pred[:, 2:], target[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    overlap = wh[:, 0] * wh[:, 1]\n",
        "\n",
        "    # union\n",
        "    ap = (pred[:, 2] - pred[:, 0]) * (pred[:, 3] - pred[:, 1])\n",
        "    ag = (target[:, 2] - target[:, 0]) * (target[:, 3] - target[:, 1])\n",
        "    union = ap + ag - overlap + eps\n",
        "\n",
        "    ious = overlap / union\n",
        "\n",
        "    # enclose area\n",
        "    enclose_x1y1 = torch.min(pred[:, :2], target[:, :2])\n",
        "    enclose_x2y2 = torch.max(pred[:, 2:], target[:, 2:])\n",
        "    enclose_wh = (enclose_x2y2 - enclose_x1y1).clamp(min=0)\n",
        "\n",
        "    cw = enclose_wh[:, 0]\n",
        "    ch = enclose_wh[:, 1]\n",
        "\n",
        "    c2 = cw**2 + ch**2 + eps\n",
        "\n",
        "    b1_x1, b1_y1 = pred[:, 0], pred[:, 1]\n",
        "    b1_x2, b1_y2 = pred[:, 2], pred[:, 3]\n",
        "    b2_x1, b2_y1 = target[:, 0], target[:, 1]\n",
        "    b2_x2, b2_y2 = target[:, 2], target[:, 3]\n",
        "\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
        "\n",
        "    left = ((b2_x1 + b2_x2) - (b1_x1 + b1_x2))**2 / 4\n",
        "    right = ((b2_y1 + b2_y2) - (b1_y1 + b1_y2))**2 / 4\n",
        "    rho2 = left + right\n",
        "\n",
        "    factor = 4 / math.pi**2\n",
        "    v = factor * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        alpha = (ious > 0.5).float() * v / (1 - ious + v)\n",
        "\n",
        "    # CIoU\n",
        "    cious = ious - (rho2 / c2 + alpha * v)\n",
        "    loss = 1 - cious.clamp(min=-1.0, max=1.0)\n",
        "    if reduction == 'mean':\n",
        "        loss = loss.mean()\n",
        "    elif reduction == 'sum':\n",
        "        loss = loss.sum()\n",
        "    elif reduction == 'none':\n",
        "        pass\n",
        "    return loss\n",
        "\n",
        "\n",
        "class CIoULoss(nn.Module):\n",
        "\n",
        "    def __init__(self, eps=1e-6, reduction='mean', loss_weight=1.0):\n",
        "        super(CIoULoss, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.reduction = reduction\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "    def forward(self,\n",
        "                pred,\n",
        "                target,\n",
        "                weight=None,\n",
        "                avg_factor=None,\n",
        "                reduction_override=None,\n",
        "                **kwargs):\n",
        "        if weight is not None and not torch.any(weight > 0):\n",
        "            if pred.dim() == weight.dim() + 1:\n",
        "                weight = weight.unsqueeze(1)\n",
        "            return (pred * weight).sum()  # 0\n",
        "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
        "        reduction = (\n",
        "            reduction_override if reduction_override else self.reduction)\n",
        "        if weight is not None and weight.dim() > 1:\n",
        "            assert weight.shape == pred.shape\n",
        "            weight = weight.mean(-1)\n",
        "        loss = self.loss_weight * ciou_loss(\n",
        "            pred,\n",
        "            target,\n",
        "            eps=self.eps,\n",
        "            reduction=reduction,\n",
        "        )\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8Di809VuvXAR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossMSE_YOLOv1(nn.Module):\n",
        "\n",
        "  def __init__(self, reduction = 'mean'):\n",
        "      super(LossMSE_YOLOv1, self).__init__()\n",
        "      self.mse = nn.MSELoss(reduction = reduction)\n",
        "  \n",
        "  def forward(self, prediction, target):\n",
        "    loss = self.mse(prediction[:,:2], target[:,:2]) + self.mse(torch.sqrt(prediction[:,2:4]),torch.sqrt(target[:,2:4]))\n",
        "    return loss.float()"
      ],
      "metadata": {
        "id": "NxKaRxg7xwhc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "26IFsavZReLs"
      },
      "outputs": [],
      "source": [
        "def train(model, X_train, y_train, X_val, y_val, lr, epochs, writer, path_save, loss_type = 'ciou', patience = 12, transformer = False):\n",
        "  train_dataloader = dataloader(X_train, y_train)\n",
        "  val_dataloader = dataloader(X_val, y_val)\n",
        "\n",
        "  if loss_type == 'ciou':\n",
        "    criterion = CIoULoss()\n",
        "  elif loss_type == 'mse':\n",
        "    criterion = LossMSE_YOLOv1()\n",
        "  # metric = generalized_iou_loss\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  sched = ReduceLROnPlateau(opt, threshold=1e-4, min_lr=1e-7, patience = patience)\n",
        "\n",
        "  patience, trials = 25, 0\n",
        "  val_loss_min = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print('======== Epoch {:} ========'.format(epoch + 1))\n",
        "\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
        "      x_batch = batch[0].cuda()\n",
        "      y_batch = batch[1].cuda()\n",
        "      opt.zero_grad()\n",
        "      out = model(x_batch)\n",
        "      if transformer:\n",
        "        num_fea = out.size(2)\n",
        "        out = out.view(-1, num_fea)\n",
        "        y_batch = y_batch.view(-1, num_fea)\n",
        "      loss = criterion(out, y_batch)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "\n",
        "      \n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_iou_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(val_dataloader):\n",
        "        x_val = batch[0].cuda()\n",
        "        y_val = batch[1].cuda()\n",
        "        out = model(x_val)\n",
        "        if transformer:\n",
        "          num_fea = out.size(2)\n",
        "          out = out.view(-1, num_fea)\n",
        "          y_val = y_val.view(-1, num_fea)\n",
        "        loss = criterion(out, y_val)\n",
        "        total_val_loss += loss.item()\n",
        "      \n",
        "      val_loss = total_val_loss/len(val_dataloader)\n",
        "      sched.step(val_loss)\n",
        "\n",
        "    if val_loss < val_loss_min:\n",
        "        trials = 0\n",
        "        torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "            }, path_save)\n",
        "        val_loss_min = val_loss\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(f'Early stopping on epoch {epoch + 1}')\n",
        "            break\n",
        "    \n",
        "\n",
        "    print('Epoch[{}/{}]: train_loss: {:.10f}, val_loss:{:.10f}'.format(epoch, epochs, train_loss, val_loss))\n",
        "    #add to tensorboard\n",
        "    writer.add_scalars(f'loss traing', {\n",
        "        'train': train_loss,\n",
        "        'val': val_loss,\n",
        "    }, epoch)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3-seq"
      ],
      "metadata": {
        "id": "yyWEgxpsqvhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tạo dataset\n",
        "num_frames_input = 4\n",
        "list_ID = create_list_ID(coco_annotation, coco_test_annotation)\n",
        "for key in list_ID.keys():\n",
        "  list_ID[key].sort(key=vid_path)\n",
        "  b = [a[0] for a in list_ID[key]]\n",
        "  list_ID[key] = b\n",
        "train_list = create_train_list(list_ID, num_frames_input)\n",
        "x_scale, y_scale = create_dataset(train_list, coco_annotation, coco_test_annotation)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scale, y_scale, test_size=0.2, shuffle=True, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle = True, random_state=42)"
      ],
      "metadata": {
        "id": "PO7eeMX1qy3h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('./output/Lstm_Attetion1{}seq'.format(num_frames_input))\n",
        "path_save = './output/Lstm_Attetion1.bestweight{}'.format(num_frames_input)"
      ],
      "metadata": {
        "id": "AAGZRz4sq-3i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 4 \n",
        "hidden_dim = 256\n",
        "output_dim = 4\n",
        "\n",
        "lr = 0.001\n",
        "n_epochs = 200\n",
        "model = LSTM_Attention(input_dim, hidden_dim, output_dim)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "qbqUh5hTrHt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(model, x_train, y_train, x_val, y_val,lr = lr, epochs = n_epochs, writer = writer, path_save = path_save)"
      ],
      "metadata": {
        "id": "obKwhq59raSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(path_save)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "UYEA7-gh1V-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.as_tensor(x_test).cuda())\n",
        "  y_true = torch.as_tensor(y_test).cuda()\n",
        "  loss = nn.MSELoss()\n",
        "  print('mseLoss = {}'.format(loss(batch_xyxy2cxcy(pred), batch_xyxy2cxcy(y_true))))\n",
        "  print('IOU = {}'.format(generalized_iou(y_true, pred)))"
      ],
      "metadata": {
        "id": "_LjEKSiQsBMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5-seq"
      ],
      "metadata": {
        "id": "HIeQd8wy7cYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tạo dataset\n",
        "num_frames_input = 6\n",
        "list_ID = create_list_ID(coco_annotation, coco_test_annotation)\n",
        "for key in list_ID.keys():\n",
        "  list_ID[key].sort(key=vid_path)\n",
        "  b = [a[0] for a in list_ID[key]]\n",
        "  list_ID[key] = b\n",
        "train_list = create_train_list(list_ID, num_frames_input)\n",
        "x_scale, y_scale = create_dataset(train_list, coco_annotation, coco_test_annotation)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scale, y_scale, test_size=0.2, shuffle=True, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle = True, random_state=42)"
      ],
      "metadata": {
        "id": "YErZuScI7cYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('./output/Lstm_Attetion{}seq'.format(num_frames_input))\n",
        "path_save = './output/Lstm_Attetion.bestweight{}'.format(num_frames_input)"
      ],
      "metadata": {
        "id": "Qo65uHV77cYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 4 \n",
        "hidden_dim = 256\n",
        "output_dim = 4\n",
        "\n",
        "lr = 0.001\n",
        "n_epochs = 200\n",
        "model = LSTM_Attention(input_dim, hidden_dim, output_dim)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "ARGo2PnV7cY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(model, x_train, y_train, x_val, y_val,lr = lr, epochs = n_epochs, writer = writer, path_save = path_save)"
      ],
      "metadata": {
        "id": "ny_DSliT7cY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(path_save)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "HvM0nGog7caL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.as_tensor(x_test).cuda())\n",
        "  y_true = torch.as_tensor(y_test).cuda()\n",
        "  loss = nn.MSELoss()\n",
        "  print('mseLoss = {}'.format(loss(batch_xyxy2cxcy(pred), batch_xyxy2cxcy(y_true))))\n",
        "  print('IOU = {}'.format(generalized_iou(y_true, pred)))"
      ],
      "metadata": {
        "id": "7XQiX3y-7caM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7-seq"
      ],
      "metadata": {
        "id": "78BwrOig7cnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tạo dataset\n",
        "num_frames_input = 8\n",
        "list_ID = create_list_ID(coco_annotation, coco_test_annotation)\n",
        "for key in list_ID.keys():\n",
        "  list_ID[key].sort(key=vid_path)\n",
        "  b = [a[0] for a in list_ID[key]]\n",
        "  list_ID[key] = b\n",
        "train_list = create_train_list(list_ID, num_frames_input)\n",
        "x_scale, y_scale = create_dataset(train_list, coco_annotation, coco_test_annotation)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scale, y_scale, test_size=0.2, shuffle=True, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle = True, random_state=42)"
      ],
      "metadata": {
        "id": "HJ3wZndQ7cna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('./output/Lstm_Attetion{}seq'.format(num_frames_input))\n",
        "path_save = './output/Lstm_Attetion.bestweight{}'.format(num_frames_input)"
      ],
      "metadata": {
        "id": "2HhmZ57X7cnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 4 \n",
        "hidden_dim = 256\n",
        "output_dim = 4\n",
        "\n",
        "lr = 0.001\n",
        "n_epochs = 200\n",
        "model = LSTM_Attention(input_dim, hidden_dim, output_dim)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "BxjI82pA7cnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(model, x_train, y_train, x_val, y_val,lr = lr, epochs = n_epochs, writer = writer, path_save = path_save)"
      ],
      "metadata": {
        "id": "_26EcaCJ7cnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(path_save)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "LMfGk7l77cow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.as_tensor(x_test).cuda())\n",
        "  y_true = torch.as_tensor(y_test).cuda()\n",
        "  loss = nn.MSELoss()\n",
        "  print('mseLoss = {}'.format(loss(batch_xyxy2cxcy(pred), batch_xyxy2cxcy(y_true))))\n",
        "  print('IOU = {}'.format(generalized_iou(y_true, pred)))"
      ],
      "metadata": {
        "id": "YwiO7VZa7cox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9-seq"
      ],
      "metadata": {
        "id": "jT4GgqO47c17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tạo dataset\n",
        "num_frames_input = 10\n",
        "list_ID = create_list_ID(coco_annotation, coco_test_annotation)\n",
        "for key in list_ID.keys():\n",
        "  list_ID[key].sort(key=vid_path)\n",
        "  b = [a[0] for a in list_ID[key]]\n",
        "  list_ID[key] = b\n",
        "train_list = create_train_list(list_ID, num_frames_input)\n",
        "x_scale, y_scale = create_dataset(train_list, coco_annotation, coco_test_annotation)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scale, y_scale, test_size=0.2, shuffle=True, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle = True, random_state=42)"
      ],
      "metadata": {
        "id": "Vuo4w7ct7c18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('./output/Lstm_Attetion{}seq'.format(num_frames_input))\n",
        "path_save = './output/Lstm_Attetion.bestweight{}'.format(num_frames_input)"
      ],
      "metadata": {
        "id": "nEPeIO5U7c1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 4 \n",
        "hidden_dim = 256\n",
        "output_dim = 4\n",
        "\n",
        "lr = 0.001\n",
        "n_epochs = 200\n",
        "model = LSTM_Attention(input_dim, hidden_dim, output_dim)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "-2IM49Iu7c1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(model, x_train, y_train, x_val, y_val,lr = lr, epochs = n_epochs, writer = writer, path_save = path_save)"
      ],
      "metadata": {
        "id": "-GHA9Hh67c1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(path_save)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "tzrjqWtB7c3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.as_tensor(x_test).cuda())\n",
        "  y_true = torch.as_tensor(y_test).cuda()\n",
        "  loss = nn.MSELoss()\n",
        "  print('mseLoss = {}'.format(loss(batch_xyxy2cxcy(pred), batch_xyxy2cxcy(y_true))))\n",
        "  print('IOU = {}'.format(generalized_iou(y_true, pred)))"
      ],
      "metadata": {
        "id": "jHoiFIgy7c3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}